#!/usr/bin/env python
# coding: utf-8

# In[ ]:


import gym
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import matplotlib.gridspec as gridspec
from time import sleep


# In[ ]:


env = gym.make('MountainCar-v0')
env.seed(0)


# ## Initialization 

# In[ ]:


n_states = 40 # discretize state space into 40 discrete states
n_actions = env.action_space.n
alpha = 0.1 # learning rate
gamma = 0.99 # discount factor
epsilon = 0.5
epsilon_decay_rate = 0.99
repeat=5
episode=1000
q_table = np.zeros((n_states, n_states, n_actions))
episode_reward=np.zeros(shape=(repeat,episode))
epsilon_decay=[]


# ## Discretize Continuous State Space

# In[ ]:


def discretize_state(state):
    env_low = env.observation_space.low
    env_high = env.observation_space.high
    env_distance = (env_high - env_low) / n_states
    p = (state - env_low) / env_distance
    return np.floor(p).astype(int)


# ## Train Agent 

# In[ ]:


for r in range(repeat):
    for i_episode in range(episode):
        print("="*50)
        print("Repeat:%d Episode:%d"%(r,i_episode))
        print("="*50)
        state = env.reset()
        done = False
        total_reward=0
        while not done:
            state_discrete = discretize_state(state)
            # Choose an action using epsilon-greedy policy
            if np.random.uniform() < epsilon:
                action = env.action_space.sample()
            else:
                action = np.argmax(q_table[state_discrete[0], state_discrete[1]])

            # Take the chosen action and observe the next state and reward
            next_state, reward, done, _ = env.step(action)
            next_state_discrete = discretize_state(next_state)
            print(f"Current State:{np.round(state,4)},Action:{action},Reward:{reward},Next State:{np.round(next_state,4)}, Terminal:{done}")
            # Update the Q-table using Q-learning formula
            q_next_max = np.max(q_table[next_state_discrete[0], next_state_discrete[1]])
            q_table[state_discrete[0], state_discrete[1], action] += alpha * (reward + gamma * q_next_max - q_table[state_discrete[0], state_discrete[1], action])
            total_reward+=reward

            state = next_state
        episode_reward[r][i_episode]=total_reward
        # Decay epsilon
        epsilon *= epsilon_decay_rate
        epsilon_decay.append(epsilon)


# ## Compute Average Reward and Plot Diagram

# In[ ]:


Mean_average=episode_reward.mean(axis=0)
Moving_average=pd.DataFrame(Mean_average)
Moving_average=Moving_average.rolling(300).mean()


# In[ ]:


fig = plt.figure(tight_layout=True,figsize=(10,4))
gs = gridspec.GridSpec(1, 2,width_ratios=[5,3.5])

ax = fig.add_subplot(gs[0, 0])
ax.plot(Moving_average,label="Q-learning with alpha = 0.1",c='r')
plt.title("Moving Average Reward\nMountain Car Environment (5 Repeats with 1000 Episodes)")
plt.legend()
plt.xlabel("Episode")
plt.ylabel("Reward(Number of Steps)")
plt.grid()

ax = fig.add_subplot(gs[0, 1])
ax.plot(epsilon_decay[:1000])
ax.set_ylabel('Epsilon')
ax.set_xlabel('Episode')
plt.title("Epsilon Decaying")
plt.grid()


# ## Test Agent

# In[ ]:


state = env.reset()
done = False
while not done:
    state_discrete = discretize_state(state)
    action = np.argmax(q_table[state_discrete[0], state_discrete[1]])
    next_state, reward, done, _ = env.step(action)
    state = next_state
    env.render()
sleep(5)
env.close()

